{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7BLx69FmLXkO",
    "tags": []
   },
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ravi/miniconda3/envs/transformers/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# warnings\n",
    "from warnings import simplefilter\n",
    "simplefilter(\"ignore\", category=FutureWarning)\n",
    "simplefilter(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# import libraries\n",
    "seed=42\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "import random\n",
    "random.seed(seed)\n",
    "import numpy as np\n",
    "np.random.seed(seed)\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# preprocess\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets\n",
    "\n",
    "# train\n",
    "import evaluate\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "# prediction\n",
    "import torch\n",
    "\n",
    "# custom built functions\n",
    "from logs.get_logs import setup_logger\n",
    "from dataPrep.get_data_fold import data_read\n",
    "from models.TFs.Transformers_model import BatchTokenize, BatchTokenizeCombine, Transformers_train, Transformers_predict\n",
    "from utils.utils import set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "07Ay4WkPLaG3"
   },
   "outputs": [],
   "source": [
    "# functions go here\n",
    "\n",
    "def getModelType(model_select):\n",
    "    \n",
    "    if model_select==\"RoBERTa\": # batch size 10\n",
    "        model_type=\"roberta-large\" # https://huggingface.co/models?sort=downloads&search=roberta\n",
    "    elif model_select==\"Longformer\": # batch size 1\n",
    "        model_type=\"allenai/longformer-large-4096\" # https://huggingface.co/models?sort=downloads&search=longformer-large\n",
    "    elif model_select==\"OpenAIGPT2\": # batch size 6\n",
    "        model_type=\"gpt2\" # https://huggingface.co/models?sort=downloads&search=gpt2\n",
    "    \n",
    "    return model_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-09 12:58:24,797 | 3823710521.py: 23: <cell line: 23>() | INFO: =========================================================\n",
      "2024-04-09 12:58:24,798 | 3823710521.py: 24: <cell line: 24>() | INFO: ==================== New execution ======================\n",
      "2024-04-09 12:58:24,799 | 3823710521.py: 25: <cell line: 25>() | INFO: =========================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_dir_fname: /home/ravi/raviProject/DataModelsResults//Results/OpenAIGPT2_1/OpenAIGPT2_Tokenize_Train_Test_1.log\n"
     ]
    }
   ],
   "source": [
    "## inputs\n",
    "ite = 1\n",
    "\n",
    "# Choose model\n",
    "model_select = \"OpenAIGPT2\" # Options: RoBERTa, Longformer, OpenAIGPT2\n",
    "model_type = getModelType(model_select)\n",
    "\n",
    "# Choose\n",
    "model_tokenize=0\n",
    "TokenizeCombine=0\n",
    "model_train=0\n",
    "model_predict=0\n",
    "\n",
    "# logger\n",
    "task = \"_Tokenize_Train_Test_\"+str(ite) # Train Test\n",
    "taskName = model_select + task\n",
    "root_dir = '/home/ravi/raviProject/DataModelsResults/'\n",
    "model_folder = root_dir + \"/Results/\" + model_select + \"_\" + str(ite) + \"/\"\n",
    "log_dir_fname = model_folder + taskName +\".log\"\n",
    "print(\"log_dir_fname: {}\".format(log_dir_fname))\n",
    "logger = setup_logger(log_dir_fname=log_dir_fname)\n",
    "\n",
    "logger.info(\"=========================================================\")  \n",
    "logger.info(\"==================== New execution ======================\")\n",
    "logger.info(\"=========================================================\")\n",
    "execution_st = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-09 12:58:24,810 | 1269898119.py: 3: <cell line: 3>() | INFO: Get inputs data\n",
      "2024-04-09 12:58:24,826 | 1269898119.py: 8: <cell line: 8>() | INFO: all_train_data.shape (300, 12)\n",
      "2024-04-09 12:58:24,830 | 1269898119.py: 29: <cell line: 29>() | INFO: Execution time 0.030343294143676758 seconds\n"
     ]
    }
   ],
   "source": [
    "# elif model_tokenize: \n",
    "# inputs\n",
    "logger.info(\"Get inputs data\")\n",
    "# Load data. Get K-Fold data. Save 5 fold indices (80% train, 20% test)\n",
    "all_train_data = pd.read_json(\"/home/ravi/raviProject/DataModelsResults/Data/V1_Labeled_300_sampled.json\", orient='records')\n",
    "all_train_data = all_train_data.drop(columns=['label'])\n",
    "all_train_data['FinalLabel'] = all_train_data['FinalLabel'].astype('int64')\n",
    "logger.info(\"all_train_data.shape {}\".format(all_train_data.shape))\n",
    "\n",
    "# format\n",
    "all_train_data = all_train_data.rename(columns={\"FinalLabel\": \"label\"})\n",
    "\n",
    "# Sample the DataFrame with replacement\n",
    "# n=2000\n",
    "val_data = all_train_data.sample(frac=0.2, random_state=42, replace=False)\n",
    "# Drop the sampled rows from the DataFrame\n",
    "train_data = all_train_data.drop(val_data.index)\n",
    "\n",
    "train_data = train_data[['text','label']]\n",
    "val_data = val_data[['text','label']]\n",
    "\n",
    "train_data.reset_index(drop=True, inplace=True)\n",
    "val_data.reset_index(drop=True)\n",
    "\n",
    "# Tokenize\n",
    "BatchTokenize(logger, model_tokenize, model_type, model_select, model_folder, train_data, val_data)\n",
    "\n",
    "# end\n",
    "logger.info(\"Execution time {} seconds\".format(time.time()-execution_st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-09 12:58:24,837 | Transformers_model.py: 189: BatchTokenizeCombine() | INFO: =========================================================\n",
      "2024-04-09 12:58:24,838 | Transformers_model.py: 190: BatchTokenizeCombine() | INFO: ========= Combine Tokenize train and val data ==========\n",
      "2024-04-09 12:58:24,838 | Transformers_model.py: 191: BatchTokenizeCombine() | INFO: =========================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-09 12:58:45,035 | Transformers_model.py: 221: BatchTokenizeCombine() | INFO: tokenized_AllTrainData is \n",
      " DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 240\n",
      "    })\n",
      "    val: Dataset({\n",
      "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 60\n",
      "    })\n",
      "})\n",
      "2024-04-09 12:58:45,035 | Transformers_model.py: 223: BatchTokenizeCombine() | INFO: Combine Tokens n time 20.195966720581055 seconds\n"
     ]
    }
   ],
   "source": [
    "# BatchTokenizeCombine\n",
    "BatchTokenizeCombine(logger, model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warnings\n",
    "from warnings import simplefilter\n",
    "simplefilter(\"ignore\", category=FutureWarning)\n",
    "simplefilter(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# import libraries\n",
    "seed=42\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "import random\n",
    "random.seed(seed)\n",
    "import numpy as np\n",
    "np.random.seed(seed)\n",
    "import time\n",
    "\n",
    "import time\n",
    "import joblib\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "import io\n",
    "import shutil\n",
    "\n",
    "# preprocess\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# preprocess\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets, load_dataset, load_from_disk\n",
    "import pandas as pd\n",
    "\n",
    "# train\n",
    "import evaluate\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch import nn\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "# prediction\n",
    "import torch\n",
    "import glob\n",
    "from natsort import natsorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-09 12:58:45,059 | 3493274202.py: 5: <cell line: 5>() | INFO: =========================================================\n",
      "2024-04-09 12:58:45,061 | 3493274202.py: 6: <cell line: 6>() | INFO: ================Finetuning on Train data=================\n",
      "2024-04-09 12:58:45,062 | 3493274202.py: 7: <cell line: 7>() | INFO: =========================================================\n",
      "2024-04-09 12:58:45,063 | 3493274202.py: 9: <cell line: 9>() | INFO: =======load Tokenize train  and val data==========\n",
      "2024-04-09 12:58:45,071 | 3493274202.py: 14: <cell line: 14>() | INFO: tokenized_AllTrainData is \n",
      " DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 240\n",
      "    })\n",
      "    val: Dataset({\n",
      "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 60\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "model_st = time.time()\n",
    "\n",
    "# del AllTrainData # to clear memory\n",
    "\n",
    "logger.info(\"=========================================================\")  \n",
    "logger.info(\"================Finetuning on Train data=================\")\n",
    "logger.info(\"=========================================================\")\n",
    "\n",
    "logger.info(\"=======load Tokenize train  and val data==========\")\n",
    "\n",
    "tokenized_AllTrainData = load_from_disk(model_folder+'/tokenized_AllTrainData')\n",
    "# tokenized_AllTrainData = load_from_disk(model_folder+'/gpt2-large-tokenized_AllTrainData') # model OOM\n",
    "\n",
    "logger.info(\"tokenized_AllTrainData is \\n {}\".format(tokenized_AllTrainData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 240\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 60\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_AllTrainData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create a batch of examples using DataCollatorWithPadding. Itâ€™s more efficient to dynamically pad the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length.\n",
    "# The next step is to load a DistilBERT tokenizer to preprocess the text field:\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_type)\n",
    "\n",
    "if model_select==\"OpenAIGPT2\": # https://github.com/huggingface/transformers/issues/3859 and https://gmihaila.github.io/tutorial_notebooks/gpt2_finetune_classification/ \n",
    "    # default to left padding\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    # Define PAD Token = EOS Token = 50256\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-09 12:58:46,713 | 2339552246.py: 1: <cell line: 1>() | INFO: ======== train =========\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"======== train =========\")\n",
    "\n",
    "# compute metric\n",
    "weighted_f1_metric = evaluate.load(\"f1\")\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return weighted_f1_metric.compute(predictions=predictions, references=labels, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-09 12:58:46,995 | 2021215553.py: 3: <cell line: 3>() | INFO: id2label is \n",
      " {1: '1', 2: '2', 3: '3'}\n"
     ]
    }
   ],
   "source": [
    "# Before you start training your model, create a map of the expected ids to their labels with id2label and label2id:\n",
    "id2label = {1: \"1\", 2: \"2\", 3: \"3\"}\n",
    "logger.info(\"id2label is \\n {}\".format(id2label))\n",
    "label2id = {'1': 1, '2': 2, '3': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-09 12:58:47,004 | 3647357061.py: 3: <cell line: 3>() | INFO: ========Training Model=========\n",
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "\n",
    "logger.info(\"========Training Model=========\")\n",
    "\n",
    "# from epoch 0\n",
    "# multi class and single label; not problem_type=\"multi_label_classification\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_type, num_labels=3, id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50257, 768)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2ForSequenceClassification(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (score): Linear(in_features=768, out_features=3, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # path to the model checkpoint from the 36th epoch\n",
    "# model_checkpoint = \"/home/ravi/UCF Dropbox/KAMALAKKANNAN RAVI/guyonDesktop/DATA_AutomatedHarmDetection/DataModelsResults/Results/OpenAIGPT2/checkpoint-288000/\"\n",
    "# # Load the model from the checkpoint\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n",
    "\n",
    "if model_select==\"OpenAIGPT2\": # https://github.com/huggingface/transformers/issues/3859 and https://gmihaila.github.io/tutorial_notebooks/gpt2_finetune_classification/ \n",
    "    # resize model embedding to match new tokenizer\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    # fix model padding token id\n",
    "    model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_folder,\n",
    "    seed=seed,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size, # to avoid OOM\n",
    "    gradient_accumulation_steps=1, # to avoid OOM\n",
    "    per_device_eval_batch_size=batch_size, # to avoid OOM\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=2,\n",
    "    save_steps=10,\n",
    "    eval_steps=10,\n",
    "    fp16=True, # to avoid OOM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CUDA_LAUNCH_BLOCKING environment variable\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_AllTrainData[\"train\"],\n",
    "    eval_dataset=tokenized_AllTrainData[\"val\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 240\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 60\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrkk-bme\u001b[0m (\u001b[33mraviucf\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ravi/raviProject/CODE/model/wandb/run-20240409_125858-38bhoero</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/raviucf/huggingface/runs/38bhoero\" target=\"_blank\">/home/ravi/raviProject/DataModelsResults//Results/OpenAIGPT2_1/</a></strong> to <a href=\"https://wandb.ai/raviucf/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ravi/miniconda3/envs/transformers/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "wandb: Network error (ReadTimeout), entering retry loop.\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"========Saving Model=========\")\n",
    "trainer.save_model()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ULMFITTrainer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
