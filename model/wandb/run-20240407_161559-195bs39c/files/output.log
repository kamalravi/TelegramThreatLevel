  0%|                                                                                                                                                    | 0/300 [00:00<?, ?it/s]2024-04-07 16:16:01,946 | Transformers_model.py: 349: compute_loss() | INFO: inputs are
 {'labels': tensor([2, 2, 1, 2], device='cuda:0'), 'input_ids': tensor([[50256, 50256, 50256, 50256, 50256,    40, 12472,   326,   262,  3071,
          9253,  2058,   284,  1657,  2582,   290, 17586,    82,   262,  2482,
            13,   220,   314,   892,  5689,   290, 12670,  6081,   290,   262,
          1854, 10925,   257,  3621,  2685,    13],
        [50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,
         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,
         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,    47,  1071,
           320,   287,  7356,   355,   499,    13],
        [ 2437,   318,   257,  1923,  4219,   597,  1180, 10117,   296,   644,
           356,   447,   247,   303,  2982,   379, 22558,   220, 19622,   220,
           220, 28416,  2147,    13,   220,   679, 33930,   284,  2222,   517,
          9975,    13,   220, 18581,   220, 34635],
        [50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 28042,
           428, 31497,   306,   366,  1197,     1,   468,  1682,   587,  5047,
            11,  5169,   290,   318,  3058,   287, 41722,  4146,    11,  1674,
          2188,   262,  2056,   582,   544, 10185]], device='cuda:0'), 'attention_mask': tensor([[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}
2024-04-07 16:16:01,947 | Transformers_model.py: 351: compute_loss() | INFO: labels are
 tensor([2, 2, 1, 2], device='cuda:0')
/home/ravi/miniconda3/envs/transformers/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|â–                                                                                                                                           | 1/300 [00:05<24:57,  5.01s/it]2024-04-07 16:16:06,952 | Transformers_model.py: 349: compute_loss() | INFO: inputs are
 {'labels': tensor([1, 3, 1, 3], device='cuda:0'), 'input_ids': tensor([[50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,
         50256, 24446,   502,   546,   340,   300,    76,    69,  5488],
        [50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,
         50256, 50256, 50256, 50256, 50256, 50256,    39,   648,   683],
        [50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,
         50256,  2949,   314,   892,   314, 11638,   340,   503,  1541],
        [ 2484,  1025,    13,  1550,    13, 26864,    13,  1400,  5964,  2622,
            13,  1400,  6131,  4179,    13,  5985,   510,  2253,     0]],
       device='cuda:0'), 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],
       device='cuda:0')}
2024-04-07 16:16:06,953 | Transformers_model.py: 351: compute_loss() | INFO: labels are
 tensor([1, 3, 1, 3], device='cuda:0')
/opt/conda/conda-bld/pytorch_1656352630480/work/aten/src/ATen/native/cuda/Loss.cu:271: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/conda/conda-bld/pytorch_1656352630480/work/aten/src/ATen/native/cuda/Loss.cu:271: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
Traceback (most recent call last):
  File "/home/ravi/raviProject/CODE/model/transformersAWS.py", line 129, in <module>
    Transformers_train(logger, model_select, model_train, model_type, model_folder, train_data)
  File "/home/ravi/raviProject/CODE/model/models/TFs/Transformers_model.py", line 370, in Transformers_train
    trainer.train()
  File "/home/ravi/miniconda3/envs/transformers/lib/python3.10/site-packages/transformers/trainer.py", line 1422, in train
    tr_loss_step = self.training_step(model, inputs)
  File "/home/ravi/miniconda3/envs/transformers/lib/python3.10/site-packages/transformers/trainer.py", line 2021, in training_step
    self.scaler.scale(loss).backward()
  File "/home/ravi/miniconda3/envs/transformers/lib/python3.10/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/ravi/miniconda3/envs/transformers/lib/python3.10/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.