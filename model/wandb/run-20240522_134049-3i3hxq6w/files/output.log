
  0%|                                                                                                 | 1/7200 [00:05<10:41:58,  5.35s/it]The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 358
  Batch size = 5



Configuration saved in /home/ravi/raviProject/DataModelsResults//Results/RoBERTa_4/checkpoint-1/config.json
Model weights saved in /home/ravi/raviProject/DataModelsResults//Results/RoBERTa_4/checkpoint-1/pytorch_model.bin
tokenizer config file saved in /home/ravi/raviProject/DataModelsResults//Results/RoBERTa_4/checkpoint-1/tokenizer_config.json
Special tokens file saved in /home/ravi/raviProject/DataModelsResults//Results/RoBERTa_4/checkpoint-1/special_tokens_map.json
{'eval_loss': 1.18635892868042, 'eval_f1': 0.8792216445400292, 'eval_runtime': 8.4719, 'eval_samples_per_second': 42.257, 'eval_steps_per_second': 2.125, 'epoch': 0.01}
Deleting older checkpoint [/home/ravi/raviProject/DataModelsResults/Results/RoBERTa_4/checkpoint-1913_epoch_50] due to args.save_total_limit
  0%|                                                                                                 | 2/7200 [00:20<21:38:49, 10.83s/it]The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 358
  Batch size = 5

  File "/home/ravi/raviProject/CODE/model/transformersMain.py", line 141, in <module>                      | 8/18 [00:03<00:05,  1.91it/s]
    Transformers_train(logger, model_select, model_train, model_type, model_folder)
  File "/home/ravi/raviProject/CODE/model/models/TFs/Transformers_model.py", line 362, in Transformers_train
    trainer.train(resume_from_checkpoint=model_folder)
  File "/home/ravi/miniconda3/envs/transformers/lib/python3.10/site-packages/transformers/trainer.py", line 1497, in train
    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
  File "/home/ravi/miniconda3/envs/transformers/lib/python3.10/site-packages/transformers/trainer.py", line 1624, in _maybe_log_save_evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
  File "/home/ravi/miniconda3/envs/transformers/lib/python3.10/site-packages/transformers/trainer.py", line 2284, in evaluate
    output = eval_loop(
  File "/home/ravi/miniconda3/envs/transformers/lib/python3.10/site-packages/transformers/trainer.py", line 2458, in evaluation_loop
    loss, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
  File "/home/ravi/miniconda3/envs/transformers/lib/python3.10/site-packages/transformers/trainer.py", line 2671, in prediction_step
    loss, outputs = self.compute_loss(model, inputs, return_outputs=True)
  File "/home/ravi/miniconda3/envs/transformers/lib/python3.10/site-packages/transformers/trainer.py", line 2043, in compute_loss
    outputs = model(**inputs)
  File "/home/ravi/miniconda3/envs/transformers/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ravi/miniconda3/envs/transformers/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py", line 167, in forward
    replicas = self.replicate(self.module, self.device_ids[:len(inputs)])
  File "/home/ravi/miniconda3/envs/transformers/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py", line 172, in replicate
    return replicate(module, device_ids, not torch.is_grad_enabled())
  File "/home/ravi/miniconda3/envs/transformers/lib/python3.10/site-packages/torch/nn/parallel/replicate.py", line 115, in replicate
    replica = module._replicate_for_data_parallel()
  File "/home/ravi/miniconda3/envs/transformers/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1972, in _replicate_for_data_parallel
    replica._modules = replica._modules.copy()
  File "/home/ravi/miniconda3/envs/transformers/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1220, in __setattr__
    if isinstance(value, Parameter):
KeyboardInterrupt