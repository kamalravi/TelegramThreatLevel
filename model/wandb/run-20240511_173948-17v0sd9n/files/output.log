
  0%|                                                                                                  | 1/2400 [00:05<3:29:46,  5.25s/it]The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 120
  Batch size = 5
 83%|█████████████████████████████████████████████████████████████████████████████████████▊                 | 5/6 [00:01<00:00,  2.54it/s]
Configuration saved in /home/ravi/raviProject/DataModelsResults//Results/RoBERTa_2/checkpoint-1/config.json
Model weights saved in /home/ravi/raviProject/DataModelsResults//Results/RoBERTa_2/checkpoint-1/pytorch_model.bin
tokenizer config file saved in /home/ravi/raviProject/DataModelsResults//Results/RoBERTa_2/checkpoint-1/tokenizer_config.json
Special tokens file saved in /home/ravi/raviProject/DataModelsResults//Results/RoBERTa_2/checkpoint-1/special_tokens_map.json
  0%|                                                                                                  | 2/2400 [00:13<4:40:04,  7.01s/it]The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 120
  Batch size = 5
{'eval_loss': 1.132493495941162, 'eval_f1': 0.1453290870488323, 'eval_runtime': 2.6796, 'eval_samples_per_second': 44.783, 'eval_steps_per_second': 2.239, 'epoch': 0.08}
Configuration saved in /home/ravi/raviProject/DataModelsResults//Results/RoBERTa_2/checkpoint-2/config.json
Model weights saved in /home/ravi/raviProject/DataModelsResults//Results/RoBERTa_2/checkpoint-2/pytorch_model.bin
tokenizer config file saved in /home/ravi/raviProject/DataModelsResults//Results/RoBERTa_2/checkpoint-2/tokenizer_config.json
Special tokens file saved in /home/ravi/raviProject/DataModelsResults//Results/RoBERTa_2/checkpoint-2/special_tokens_map.json
  0%|                                                                                                  | 3/2400 [00:21<5:01:49,  7.56s/it]The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 120
  Batch size = 5
 83%|█████████████████████████████████████████████████████████████████████████████████████▊                 | 5/6 [00:01<00:00,  2.50it/s]
Configuration saved in /home/ravi/raviProject/DataModelsResults//Results/RoBERTa_2/checkpoint-3/config.json
Model weights saved in /home/ravi/raviProject/DataModelsResults//Results/RoBERTa_2/checkpoint-3/pytorch_model.bin
tokenizer config file saved in /home/ravi/raviProject/DataModelsResults//Results/RoBERTa_2/checkpoint-3/tokenizer_config.json
Special tokens file saved in /home/ravi/raviProject/DataModelsResults//Results/RoBERTa_2/checkpoint-3/special_tokens_map.json
Deleting older checkpoint [/home/ravi/raviProject/DataModelsResults/Results/RoBERTa_2/checkpoint-1] due to args.save_total_limit
  0%|▏                                                                                                 | 4/2400 [00:30<5:26:52,  8.19s/it]The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 120
  Batch size = 5
 50%|███████████████████████████████████████████████████▌                                                   | 3/6 [00:00<00:00,  3.14it/s]

Configuration saved in /home/ravi/raviProject/DataModelsResults//Results/RoBERTa_2/checkpoint-4/config.json
Model weights saved in /home/ravi/raviProject/DataModelsResults//Results/RoBERTa_2/checkpoint-4/pytorch_model.bin
tokenizer config file saved in /home/ravi/raviProject/DataModelsResults//Results/RoBERTa_2/checkpoint-4/tokenizer_config.json
Special tokens file saved in /home/ravi/raviProject/DataModelsResults//Results/RoBERTa_2/checkpoint-4/special_tokens_map.json
Deleting older checkpoint [/home/ravi/raviProject/DataModelsResults/Results/RoBERTa_2/checkpoint-2] due to args.save_total_limit
  0%|▏                                                                                                 | 5/2400 [00:39<5:32:34,  8.33s/it]The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 120
  Batch size = 5
 33%|██████████████████████████████████▎                                                                    | 2/6 [00:00<00:00,  4.47it/s]

Configuration saved in /home/ravi/raviProject/DataModelsResults//Results/RoBERTa_2/checkpoint-5/config.json
Model weights saved in /home/ravi/raviProject/DataModelsResults//Results/RoBERTa_2/checkpoint-5/pytorch_model.bin
tokenizer config file saved in /home/ravi/raviProject/DataModelsResults//Results/RoBERTa_2/checkpoint-5/tokenizer_config.json
Special tokens file saved in /home/ravi/raviProject/DataModelsResults//Results/RoBERTa_2/checkpoint-5/special_tokens_map.json
Deleting older checkpoint [/home/ravi/raviProject/DataModelsResults/Results/RoBERTa_2/checkpoint-3] due to args.save_total_limit
  0%|▏                                                                                                 | 6/2400 [00:48<5:39:16,  8.50s/it]The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 120
  Batch size = 5

Configuration saved in /home/ravi/raviProject/DataModelsResults//Results/RoBERTa_2/checkpoint-6/config.json
{'eval_loss': 1.108360767364502, 'eval_f1': 0.20530753968253965, 'eval_runtime': 2.7376, 'eval_samples_per_second': 43.834, 'eval_steps_per_second': 2.192, 'epoch': 0.25}
Model weights saved in /home/ravi/raviProject/DataModelsResults//Results/RoBERTa_2/checkpoint-6/pytorch_model.bin
tokenizer config file saved in /home/ravi/raviProject/DataModelsResults//Results/RoBERTa_2/checkpoint-6/tokenizer_config.json
Special tokens file saved in /home/ravi/raviProject/DataModelsResults//Results/RoBERTa_2/checkpoint-6/special_tokens_map.json
Deleting older checkpoint [/home/ravi/raviProject/DataModelsResults/Results/RoBERTa_2/checkpoint-4] due to args.save_total_limit
  0%|▎                                                                                                 | 7/2400 [00:56<5:41:28,  8.56s/it]The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 120
  Batch size = 5

Configuration saved in /home/ravi/raviProject/DataModelsResults//Results/RoBERTa_2/checkpoint-7/config.json
{'eval_loss': 1.1055824756622314, 'eval_f1': 0.34444829479173816, 'eval_runtime': 2.716, 'eval_samples_per_second': 44.182, 'eval_steps_per_second': 2.209, 'epoch': 0.29}
Model weights saved in /home/ravi/raviProject/DataModelsResults//Results/RoBERTa_2/checkpoint-7/pytorch_model.bin
tokenizer config file saved in /home/ravi/raviProject/DataModelsResults//Results/RoBERTa_2/checkpoint-7/tokenizer_config.json
Special tokens file saved in /home/ravi/raviProject/DataModelsResults//Results/RoBERTa_2/checkpoint-7/special_tokens_map.json
Deleting older checkpoint [/home/ravi/raviProject/DataModelsResults/Results/RoBERTa_2/checkpoint-5] due to args.save_total_limit
  0%|▎                                                                                                 | 8/2400 [01:05<5:42:30,  8.59s/it]The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 120
  Batch size = 5

Configuration saved in /home/ravi/raviProject/DataModelsResults//Results/RoBERTa_2/checkpoint-8/config.json
{'eval_loss': 1.100419521331787, 'eval_f1': 0.39175123201261575, 'eval_runtime': 2.7479, 'eval_samples_per_second': 43.669, 'eval_steps_per_second': 2.183, 'epoch': 0.33}
Model weights saved in /home/ravi/raviProject/DataModelsResults//Results/RoBERTa_2/checkpoint-8/pytorch_model.bin
tokenizer config file saved in /home/ravi/raviProject/DataModelsResults//Results/RoBERTa_2/checkpoint-8/tokenizer_config.json
Special tokens file saved in /home/ravi/raviProject/DataModelsResults//Results/RoBERTa_2/checkpoint-8/special_tokens_map.json
Deleting older checkpoint [/home/ravi/raviProject/DataModelsResults/Results/RoBERTa_2/checkpoint-6] due to args.save_total_limit
  0%|▎                                                                                                 | 9/2400 [01:14<5:43:34,  8.62s/it]The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 120
  Batch size = 5

Configuration saved in /home/ravi/raviProject/DataModelsResults//Results/RoBERTa_2/checkpoint-9/config.json
{'eval_loss': 1.0951206684112549, 'eval_f1': 0.4009469636190538, 'eval_runtime': 2.7188, 'eval_samples_per_second': 44.136, 'eval_steps_per_second': 2.207, 'epoch': 0.38}
Model weights saved in /home/ravi/raviProject/DataModelsResults//Results/RoBERTa_2/checkpoint-9/pytorch_model.bin
tokenizer config file saved in /home/ravi/raviProject/DataModelsResults//Results/RoBERTa_2/checkpoint-9/tokenizer_config.json
Special tokens file saved in /home/ravi/raviProject/DataModelsResults//Results/RoBERTa_2/checkpoint-9/special_tokens_map.json
Deleting older checkpoint [/home/ravi/raviProject/DataModelsResults/Results/RoBERTa_2/checkpoint-7] due to args.save_total_limit
  0%|▍                                                                                                | 10/2400 [01:23<5:45:21,  8.67s/it]The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 120
  Batch size = 5

Configuration saved in /home/ravi/raviProject/DataModelsResults//Results/RoBERTa_2/checkpoint-10/config.json
{'eval_loss': 1.092725396156311, 'eval_f1': 0.4925249450363606, 'eval_runtime': 2.7401, 'eval_samples_per_second': 43.794, 'eval_steps_per_second': 2.19, 'epoch': 0.42}
Model weights saved in /home/ravi/raviProject/DataModelsResults//Results/RoBERTa_2/checkpoint-10/pytorch_model.bin
tokenizer config file saved in /home/ravi/raviProject/DataModelsResults//Results/RoBERTa_2/checkpoint-10/tokenizer_config.json
Special tokens file saved in /home/ravi/raviProject/DataModelsResults//Results/RoBERTa_2/checkpoint-10/special_tokens_map.json
Deleting older checkpoint [/home/ravi/raviProject/DataModelsResults/Results/RoBERTa_2/checkpoint-8] due to args.save_total_limit
  0%|▍                                                                                                | 11/2400 [01:31<5:44:33,  8.65s/it]The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 120
  Batch size = 5
Configuration saved in /home/ravi/raviProject/DataModelsResults//Results/RoBERTa_2/checkpoint-11/config.json
Configuration saved in /home/ravi/raviProject/DataModelsResults//Results/RoBERTa_2/checkpoint-11/config.json
{'eval_loss': 1.0907963514328003, 'eval_f1': 0.45397509578544065, 'eval_runtime': 2.721, 'eval_samples_per_second': 44.102, 'eval_steps_per_second': 2.205, 'epoch': 0.46}
  Batch size = 5heckpoint [/home/ravi/raviProject/DataModelsResults/Results/RoBERTa_2/checkpoint-9] due to args.save_total_limit
  Batch size = 5heckpoint [/home/ravi/raviProject/DataModelsResults/Results/RoBERTa_2/checkpoint-9] due to args.save_total_limit
tokenizer config file saved in /home/ravi/raviProject/DataModelsResults//Results/RoBERTa_2/checkpoint-12/tokenizer_config.jsonit
tokenizer config file saved in /home/ravi/raviProject/DataModelsResults//Results/RoBERTa_2/checkpoint-12/tokenizer_config.jsonit
{'eval_loss': 1.0874167680740356, 'eval_f1': 0.3034557579659648, 'eval_runtime': 2.753, 'eval_samples_per_second': 43.588, 'eval_steps_per_second': 2.179, 'epoch': 0.5}
 50%|███████████████████████████████████████████████████▌                                                   | 3/6 [00:00<00:00,  3.15it/s]
Configuration saved in /home/ravi/raviProject/DataModelsResults//Results/RoBERTa_2/checkpoint-13/config.json
Configuration saved in /home/ravi/raviProject/DataModelsResults//Results/RoBERTa_2/checkpoint-13/config.json
{'eval_loss': 1.0849087238311768, 'eval_f1': 0.31322317956718554, 'eval_runtime': 2.7115, 'eval_samples_per_second': 44.256, 'eval_steps_per_second': 2.213, 'epoch': 0.54}
Deleting older checkpoint [/home/ravi/raviProject/DataModelsResults/Results/RoBERTa_2/checkpoint-11] due to args.save_total_limit
  File "/home/ravi/miniconda3/envs/transformers/lib/python3.10/site-packages/torch/_utils.py", line 461, in reraiseine 178, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/ravi/miniconda3/envs/transformers/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py", line 86, in parallel_apply
    output.reraise()
  File "/home/ravi/miniconda3/envs/transformers/lib/python3.10/site-packages/torch/_utils.py", line 461, in reraiseine 178, in parallel_apply
    raise exception
RuntimeError: Caught RuntimeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/ravi/miniconda3/envs/transformers/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py", line 61, in _worker
    output = module(*input, **kwargs)
  File "/home/ravi/miniconda3/envs/transformers/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ravi/miniconda3/envs/transformers/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py", line 1209, in forward
    outputs = self.roberta(
  File "/home/ravi/miniconda3/envs/transformers/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ravi/miniconda3/envs/transformers/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py", line 851, in forward
    encoder_outputs = self.encoder(
  File "/home/ravi/miniconda3/envs/transformers/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ravi/miniconda3/envs/transformers/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py", line 527, in forward
    layer_outputs = layer_module(
  File "/home/ravi/miniconda3/envs/transformers/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ravi/miniconda3/envs/transformers/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py", line 454, in forward
    layer_output = apply_chunking_to_forward(
  File "/home/ravi/miniconda3/envs/transformers/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2928, in apply_chunking_to_forward
    return forward_fn(*input_tensors)
  File "/home/ravi/miniconda3/envs/transformers/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py", line 467, in feed_forward_chunk
    layer_output = self.output(intermediate_output, attention_output)
  File "/home/ravi/miniconda3/envs/transformers/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ravi/miniconda3/envs/transformers/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py", line 381, in forward
    hidden_states = self.LayerNorm(hidden_states + input_tensor)
  File "/home/ravi/miniconda3/envs/transformers/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ravi/miniconda3/envs/transformers/lib/python3.10/site-packages/torch/nn/modules/normalization.py", line 189, in forward
    return F.layer_norm(
  File "/home/ravi/miniconda3/envs/transformers/lib/python3.10/site-packages/torch/nn/functional.py", line 2503, in layer_norm
    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)
RuntimeError: CUDA out of memory. Tried to allocate 10.00 MiB (GPU 0; 10.91 GiB total capacity; 9.70 GiB already allocated; 5.94 MiB free; 10.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF