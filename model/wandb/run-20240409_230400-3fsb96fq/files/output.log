
  0%|                                                                                                                                                                   | 0/600 [00:00<?, ?it/s]
  0%|▎                                                                                                                                                          | 1/600 [00:03<39:54,  4.00s/it]The following columns in the evaluation set  don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 60
  Batch size = 10
  0%|▎                                                                                                                                                          | 1/600 [00:04<39:54,  4.00s/it]Saving model checkpoint to /home/ravi/raviProject/DataModelsResults//Results/OpenAIGPT2_1/checkpoint-1
Configuration saved in /home/ravi/raviProject/DataModelsResults//Results/OpenAIGPT2_1/checkpoint-1/config.json
Model weights saved in /home/ravi/raviProject/DataModelsResults//Results/OpenAIGPT2_1/checkpoint-1/pytorch_model.bin
tokenizer config file saved in /home/ravi/raviProject/DataModelsResults//Results/OpenAIGPT2_1/checkpoint-1/tokenizer_config.json
Special tokens file saved in /home/ravi/raviProject/DataModelsResults//Results/OpenAIGPT2_1/checkpoint-1/special_tokens_map.json
Deleting older checkpoint [/home/ravi/raviProject/DataModelsResults/Results/OpenAIGPT2_1/checkpoint-14] due to args.save_total_limit
  0%|▌                                                                                                                                                          | 2/600 [00:06<32:59,  3.31s/it]The following columns in the evaluation set  don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 60
  Batch size = 10
  0%|▌                                                                                                                                                          | 2/600 [00:07<32:59,  3.31s/it]Saving model checkpoint to /home/ravi/raviProject/DataModelsResults//Results/OpenAIGPT2_1/checkpoint-2
Configuration saved in /home/ravi/raviProject/DataModelsResults//Results/OpenAIGPT2_1/checkpoint-2/config.json
{'eval_loss': 1.8736356496810913, 'eval_f1': 0.2126666666666667, 'eval_runtime': 0.3693, 'eval_samples_per_second': 162.463, 'eval_steps_per_second': 5.415, 'epoch': 0.33}
Model weights saved in /home/ravi/raviProject/DataModelsResults//Results/OpenAIGPT2_1/checkpoint-2/pytorch_model.bin
tokenizer config file saved in /home/ravi/raviProject/DataModelsResults//Results/OpenAIGPT2_1/checkpoint-2/tokenizer_config.json
Special tokens file saved in /home/ravi/raviProject/DataModelsResults//Results/OpenAIGPT2_1/checkpoint-2/special_tokens_map.json
Deleting older checkpoint [/home/ravi/raviProject/DataModelsResults/Results/OpenAIGPT2_1/checkpoint-15] due to args.save_total_limit
Traceback (most recent call last):
  File "/home/ravi/raviProject/CODE/model/transformersMain.py", line 131, in <module>
    Transformers_train(logger, model_select, model_train, model_type, model_folder)
  File "/home/ravi/raviProject/CODE/model/models/TFs/Transformers_model.py", line 353, in Transformers_train
    trainer.train()
  File "/home/ravi/miniconda3/envs/transformers/lib/python3.10/site-packages/transformers/trainer.py", line 1422, in train
    tr_loss_step = self.training_step(model, inputs)
  File "/home/ravi/miniconda3/envs/transformers/lib/python3.10/site-packages/transformers/trainer.py", line 2011, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/ravi/miniconda3/envs/transformers/lib/python3.10/site-packages/transformers/trainer.py", line 2043, in compute_loss
    outputs = model(**inputs)
  File "/home/ravi/miniconda3/envs/transformers/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ravi/miniconda3/envs/transformers/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py", line 168, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/home/ravi/miniconda3/envs/transformers/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py", line 178, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/ravi/miniconda3/envs/transformers/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py", line 86, in parallel_apply
    output.reraise()
  File "/home/ravi/miniconda3/envs/transformers/lib/python3.10/site-packages/torch/_utils.py", line 461, in reraise
    raise exception
RuntimeError: Caught RuntimeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/ravi/miniconda3/envs/transformers/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py", line 61, in _worker
    output = module(*input, **kwargs)
  File "/home/ravi/miniconda3/envs/transformers/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ravi/miniconda3/envs/transformers/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 1377, in forward
    transformer_outputs = self.transformer(
  File "/home/ravi/miniconda3/envs/transformers/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ravi/miniconda3/envs/transformers/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 890, in forward
    outputs = block(
  File "/home/ravi/miniconda3/envs/transformers/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ravi/miniconda3/envs/transformers/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 395, in forward
    attn_outputs = self.attn(
  File "/home/ravi/miniconda3/envs/transformers/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ravi/miniconda3/envs/transformers/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 336, in forward
    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)
  File "/home/ravi/miniconda3/envs/transformers/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 193, in _attn
    attn_weights = torch.matmul(query, key.transpose(-1, -2))
RuntimeError: CUDA out of memory. Tried to allocate 152.00 MiB (GPU 0; 10.91 GiB total capacity; 9.68 GiB already allocated; 116.31 MiB free; 10.06 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF