  0%|                                                                                                                              | 0/4800 [00:00<?, ?it/s]Initializing global attention on CLS token...
Input ids are automatically padded from 38 to 512 to be a multiple of `config.attention_window`: 512
  0%|                                                                                                                   | 1/4800 [00:36<49:11:17, 36.90s/it]The following columns in the evaluation set  don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 60
  Batch size = 5
Initializing global attention on CLS token...
Input ids are automatically padded from 44 to 512 to be a multiple of `config.attention_window`: 512
                                                                                                                                                            Initializing global attention on CLS token...
Input ids are automatically padded from 59 to 512 to be a multiple of `config.attention_window`: 512                                 | 0/12 [00:00<?, ?it/s]
                                                                                                                                                            Initializing global attention on CLS token...
Input ids are automatically padded from 61 to 512 to be a multiple of `config.attention_window`: 512                         | 2/12 [00:06<00:33,  3.34s/it]
                                                                                                                                                            Initializing global attention on CLS token...
Input ids are automatically padded from 57 to 512 to be a multiple of `config.attention_window`: 512                         | 3/12 [00:13<00:44,  4.97s/it]
                                                                                                                                                            Initializing global attention on CLS token...
Input ids are automatically padded from 33 to 512 to be a multiple of `config.attention_window`: 512                         | 4/12 [00:21<00:46,  5.79s/it]
                                                                                                                                                            Initializing global attention on CLS token...
Input ids are automatically padded from 55 to 512 to be a multiple of `config.attention_window`: 512                         | 5/12 [00:28<00:43,  6.18s/it]
                                                                                                                                                            Initializing global attention on CLS token...
Input ids are automatically padded from 57 to 512 to be a multiple of `config.attention_window`: 512                         | 6/12 [00:35<00:39,  6.56s/it]
                                                                                                                                                            Initializing global attention on CLS token...
Input ids are automatically padded from 84 to 512 to be a multiple of `config.attention_window`: 512                         | 7/12 [00:42<00:33,  6.66s/it]
                                                                                                                                                            Initializing global attention on CLS token...
Input ids are automatically padded from 43 to 512 to be a multiple of `config.attention_window`: 512                         | 8/12 [00:49<00:27,  6.85s/it]
                                                                                                                                                            Initializing global attention on CLS token...
Input ids are automatically padded from 20 to 512 to be a multiple of `config.attention_window`: 512                         | 9/12 [00:56<00:20,  6.78s/it]
                                                                                                                                                            Initializing global attention on CLS token...
Input ids are automatically padded from 29 to 512 to be a multiple of `config.attention_window`: 512████▏                   | 10/12 [01:03<00:13,  6.94s/it]
                                                                                                                                                            Initializing global attention on CLS token...
Input ids are automatically padded from 44 to 512 to be a multiple of `config.attention_window`: 512██████████████          | 11/12 [01:10<00:06,  6.98s/it]
{'eval_loss': 1.1857508420944214, 'eval_f1': 0.29269788182831663, 'eval_runtime': 84.2952, 'eval_samples_per_second': 0.712, 'eval_steps_per_second': 0.142, 'epoch': 0.02}
  0%|                                                                                                                   | 1/4800 [02:01<49:11:17, 36.90s/it]Saving model checkpoint to /home/ravi/raviProject/DataModelsResults//Results/Longformer_1/checkpoint-1
Configuration saved in /home/ravi/raviProject/DataModelsResults//Results/Longformer_1/checkpoint-1/config.json
Model weights saved in /home/ravi/raviProject/DataModelsResults//Results/Longformer_1/checkpoint-1/pytorch_model.bin
tokenizer config file saved in /home/ravi/raviProject/DataModelsResults//Results/Longformer_1/checkpoint-1/tokenizer_config.json
Special tokens file saved in /home/ravi/raviProject/DataModelsResults//Results/Longformer_1/checkpoint-1/special_tokens_map.json
Initializing global attention on CLS token...
Input ids are automatically padded from 28 to 512 to be a multiple of `config.attention_window`: 512
  0%|                                                                                                                  | 2/4800 [02:42<119:04:12, 89.34s/it]The following columns in the evaluation set  don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 60
  Batch size = 5
Initializing global attention on CLS token...
Input ids are automatically padded from 44 to 512 to be a multiple of `config.attention_window`: 512
                                                                                                                                                            Initializing global attention on CLS token...
Input ids are automatically padded from 59 to 512 to be a multiple of `config.attention_window`: 512                                 | 0/12 [00:00<?, ?it/s]
                                                                                                                                                            Initializing global attention on CLS token...
Input ids are automatically padded from 61 to 512 to be a multiple of `config.attention_window`: 512                         | 2/12 [00:07<00:35,  3.52s/it]
                                                                                                                                                            Initializing global attention on CLS token...
Input ids are automatically padded from 57 to 512 to be a multiple of `config.attention_window`: 512                         | 3/12 [00:14<00:48,  5.35s/it]
                                                                                                                                                            Initializing global attention on CLS token...
Input ids are automatically padded from 33 to 512 to be a multiple of `config.attention_window`: 512                         | 4/12 [00:22<00:48,  6.06s/it]
                                                                                                                                                            Initializing global attention on CLS token...
Input ids are automatically padded from 55 to 512 to be a multiple of `config.attention_window`: 512                         | 5/12 [00:29<00:45,  6.52s/it]
                                                                                                                                                            Initializing global attention on CLS token...
Input ids are automatically padded from 57 to 512 to be a multiple of `config.attention_window`: 512                         | 6/12 [00:36<00:40,  6.71s/it]
                                                                                                                                                            Initializing global attention on CLS token...
Input ids are automatically padded from 84 to 512 to be a multiple of `config.attention_window`: 512                         | 7/12 [00:44<00:34,  6.93s/it]
                                                                                                                                                            Initializing global attention on CLS token...
Input ids are automatically padded from 43 to 512 to be a multiple of `config.attention_window`: 512                         | 8/12 [00:50<00:27,  6.88s/it]
                                                                                                                                                            Initializing global attention on CLS token...
Input ids are automatically padded from 20 to 512 to be a multiple of `config.attention_window`: 512                         | 9/12 [00:57<00:20,  6.73s/it]
                                                                                                                                                            Initializing global attention on CLS token...
Input ids are automatically padded from 29 to 512 to be a multiple of `config.attention_window`: 512████▏                   | 10/12 [01:04<00:13,  6.89s/it]
                                                                                                                                                            Initializing global attention on CLS token...
Input ids are automatically padded from 44 to 512 to be a multiple of `config.attention_window`: 512██████████████          | 11/12 [01:12<00:07,  7.10s/it]
{'eval_loss': 1.1718462705612183, 'eval_f1': 0.3064664163537756, 'eval_runtime': 86.8429, 'eval_samples_per_second': 0.691, 'eval_steps_per_second': 0.138, 'epoch': 0.04}
  0%|                                                                                                                  | 2/4800 [04:09<119:04:12, 89.34s/it]Saving model checkpoint to /home/ravi/raviProject/DataModelsResults//Results/Longformer_1/checkpoint-2
Configuration saved in /home/ravi/raviProject/DataModelsResults//Results/Longformer_1/checkpoint-2/config.json
Model weights saved in /home/ravi/raviProject/DataModelsResults//Results/Longformer_1/checkpoint-2/pytorch_model.bin
tokenizer config file saved in /home/ravi/raviProject/DataModelsResults//Results/Longformer_1/checkpoint-2/tokenizer_config.json
Special tokens file saved in /home/ravi/raviProject/DataModelsResults//Results/Longformer_1/checkpoint-2/special_tokens_map.json
Initializing global attention on CLS token...
Input ids are automatically padded from 33 to 512 to be a multiple of `config.attention_window`: 512
  0%|                                                                                                                 | 3/4800 [04:51<142:38:12, 107.04s/it]The following columns in the evaluation set  don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 60
  Batch size = 5
Initializing global attention on CLS token...
Input ids are automatically padded from 44 to 512 to be a multiple of `config.attention_window`: 512
                                                                                                                                                            Initializing global attention on CLS token...
Input ids are automatically padded from 59 to 512 to be a multiple of `config.attention_window`: 512                                 | 0/12 [00:00<?, ?it/s]
                                                                                                                                                            Initializing global attention on CLS token...
Input ids are automatically padded from 61 to 512 to be a multiple of `config.attention_window`: 512                         | 2/12 [00:07<00:35,  3.53s/it]
                                                                                                                                                            Initializing global attention on CLS token...
Input ids are automatically padded from 57 to 512 to be a multiple of `config.attention_window`: 512                         | 3/12 [00:14<00:45,  5.03s/it]
                                                                                                                                                            Initializing global attention on CLS token...
Input ids are automatically padded from 33 to 512 to be a multiple of `config.attention_window`: 512                         | 4/12 [00:21<00:48,  6.00s/it]
                                                                                                                                                            Initializing global attention on CLS token...
Input ids are automatically padded from 55 to 512 to be a multiple of `config.attention_window`: 512                         | 5/12 [00:29<00:45,  6.46s/it]
                                                                                                                                                            Initializing global attention on CLS token...
Input ids are automatically padded from 57 to 512 to be a multiple of `config.attention_window`: 512                         | 6/12 [00:36<00:40,  6.67s/it]
                                                                                                                                                            Initializing global attention on CLS token...
Input ids are automatically padded from 84 to 512 to be a multiple of `config.attention_window`: 512                         | 7/12 [00:43<00:33,  6.77s/it]
                                                                                                                                                            Initializing global attention on CLS token...
Input ids are automatically padded from 43 to 512 to be a multiple of `config.attention_window`: 512                         | 8/12 [00:50<00:27,  6.83s/it]
                                                                                                                                                            Initializing global attention on CLS token...
Input ids are automatically padded from 20 to 512 to be a multiple of `config.attention_window`: 512                         | 9/12 [00:57<00:21,  7.13s/it]
                                                                                                                                                            Initializing global attention on CLS token...
Input ids are automatically padded from 29 to 512 to be a multiple of `config.attention_window`: 512████▏                   | 10/12 [01:05<00:14,  7.18s/it]
                                                                                                                                                            Initializing global attention on CLS token...
Input ids are automatically padded from 44 to 512 to be a multiple of `config.attention_window`: 512██████████████          | 11/12 [01:12<00:07,  7.13s/it]
{'eval_loss': 1.1596710681915283, 'eval_f1': 0.29269788182831663, 'eval_runtime': 86.3893, 'eval_samples_per_second': 0.695, 'eval_steps_per_second': 0.139, 'epoch': 0.06}
  0%|                                                                                                                 | 3/4800 [06:17<142:38:12, 107.04s/it]Saving model checkpoint to /home/ravi/raviProject/DataModelsResults//Results/Longformer_1/checkpoint-3
Configuration saved in /home/ravi/raviProject/DataModelsResults//Results/Longformer_1/checkpoint-3/config.json
Model weights saved in /home/ravi/raviProject/DataModelsResults//Results/Longformer_1/checkpoint-3/pytorch_model.bin
tokenizer config file saved in /home/ravi/raviProject/DataModelsResults//Results/Longformer_1/checkpoint-3/tokenizer_config.json
Special tokens file saved in /home/ravi/raviProject/DataModelsResults//Results/Longformer_1/checkpoint-3/special_tokens_map.json
Deleting older checkpoint [/home/ravi/raviProject/DataModelsResults/Results/Longformer_1/checkpoint-1] due to args.save_total_limit
Initializing global attention on CLS token...
Input ids are automatically padded from 13 to 512 to be a multiple of `config.attention_window`: 512
  0%|                                                                                                                 | 4/4800 [06:59<153:46:46, 115.43s/it]The following columns in the evaluation set  don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 60
  Batch size = 5
Initializing global attention on CLS token...
